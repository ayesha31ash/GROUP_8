{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e0c4b-62d9-4bfd-8eed-e9237a4c152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017d67f4-f385-4284-9dc6-3b0734ad0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce MX450\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# BLOCK 1 â€” Setup, Imports, Device\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms as T, models\n",
    "\n",
    "import timm  # if missing: pip install timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833691fe-a0ea-44ae-bdb0-5a333e405303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing images: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# BLOCK 2 â€” Load meta.csv + Image paths\n",
    "# ============================================\n",
    "\n",
    "ROOT_DIR = r\"C:\\Users\\anama\\Documents\\Group_8\"\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"Dataset\", \"DERM7PT\")\n",
    "\n",
    "META_CSV = os.path.join(DATASET_DIR, \"meta\", \"meta.csv\")\n",
    "IMAGES_DIR = os.path.join(DATASET_DIR, \"images\")\n",
    "\n",
    "df = pd.read_csv(META_CSV)\n",
    "df = df.drop(columns=[\"case_num\", \"case_id\", \"notes\"], errors=\"ignore\")\n",
    "\n",
    "df[\"derm_fullpath\"] = df[\"derm\"].apply(lambda x: os.path.join(IMAGES_DIR, x))\n",
    "df[\"exists\"] = df[\"derm_fullpath\"].apply(os.path.exists)\n",
    "\n",
    "missing = (~df[\"exists\"]).sum()\n",
    "print(\"Missing images:\", missing)\n",
    "df = df[df[\"exists\"]].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3c20fb-b671-4a4d-b444-6a76b36c48f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['basal cell carcinoma', 'blue nevus', 'clark nevus', 'combined nevus', 'congenital nevus', 'dermal nevus', 'dermatofibroma', 'lentigo', 'melanoma (0.76 to 1.5 mm)', 'melanoma (in situ)', 'melanoma (less than 0.76 mm)', 'melanoma (more than 1.5 mm)', 'melanoma metastasis', 'melanosis', 'miscellaneous', 'recurrent nevus', 'reed or spitz nevus', 'seborrheic keratosis', 'vascular lesion']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# BLOCK 3 â€” Label Encoding\n",
    "# ============================================\n",
    "\n",
    "counts = df[\"diagnosis\"].value_counts()\n",
    "\n",
    "# Remove \"melanoma\" class if it has only 1 sample\n",
    "if \"melanoma\" in counts.index and counts[\"melanoma\"] == 1:\n",
    "    df = df[df[\"diagnosis\"] != \"melanoma\"].reset_index(drop=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"diagnosis\"])\n",
    "class_names = list(le.classes_)\n",
    "\n",
    "print(\"Classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c6a619-fc37-454e-8cd6-47f864a1870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "=== DATA LOADING & VALIDATION ===\n",
      "Loaded meta.csv â€” shape: (1011, 19)\n",
      "Missing images: 0\n",
      "Final dataset size: 1010\n",
      "Classes: 19\n",
      "Class distribution:\n",
      "diagnosis\n",
      "clark nevus                     399\n",
      "melanoma (less than 0.76 mm)    102\n",
      "reed or spitz nevus              79\n",
      "melanoma (in situ)               64\n",
      "melanoma (0.76 to 1.5 mm)        53\n",
      "seborrheic keratosis             45\n",
      "basal cell carcinoma             42\n",
      "dermal nevus                     33\n",
      "vascular lesion                  29\n",
      "blue nevus                       28\n",
      "melanoma (more than 1.5 mm)      28\n",
      "lentigo                          24\n",
      "dermatofibroma                   20\n",
      "congenital nevus                 17\n",
      "melanosis                        16\n",
      "combined nevus                   13\n",
      "miscellaneous                     8\n",
      "recurrent nevus                   6\n",
      "melanoma metastasis               4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Encoded 19 classes:\n",
      "  0: basal cell carcinoma\n",
      "  1: blue nevus\n",
      "  2: clark nevus\n",
      "  3: combined nevus\n",
      "  4: congenital nevus\n",
      "  5: dermal nevus\n",
      "  6: dermatofibroma\n",
      "  7: lentigo\n",
      "  8: melanoma (0.76 to 1.5 mm)\n",
      "  9: melanoma (in situ)\n",
      "  10: melanoma (less than 0.76 mm)\n",
      "  11: melanoma (more than 1.5 mm)\n",
      "  12: melanoma metastasis\n",
      "  13: melanosis\n",
      "  14: miscellaneous\n",
      "  15: recurrent nevus\n",
      "  16: reed or spitz nevus\n",
      "  17: seborrheic keratosis\n",
      "  18: vascular lesion\n",
      "Main samples: 1006, Metastasis: 4\n",
      "\n",
      "Final splits:\n",
      "Train: 706, Val: 152, Test: 152\n",
      "\n",
      "Creating datasets...\n",
      "=== DATASET VALIDATION ===\n",
      "Total samples: 706\n",
      "Label range: 0 to 18\n",
      "Num classes: 19\n",
      "=== DATASET VALIDATION ===\n",
      "Total samples: 152\n",
      "Label range: 0 to 18\n",
      "Num classes: 19\n",
      "=== DATASET VALIDATION ===\n",
      "Total samples: 152\n",
      "Label range: 0 to 18\n",
      "Num classes: 19\n",
      "DataLoader batch size: 16\n",
      "Train batches: 45\n",
      "\n",
      "================================================================================\n",
      "INITIALIZING ADVANCED TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Creating Hybrid CNN-Transformer Model...\n",
      "Hybrid model parameters: 110,759,763\n",
      "\n",
      "============================================================\n",
      "PHASE 1: TRAINING HYBRID MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 718\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPHASE 1: TRAINING HYBRID MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m--> 718\u001b[0m hybrid_history, hybrid_best_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_comprehensive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhybrid_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHybrid_CNN_Transformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# More epochs for convergence\u001b[39;49;00m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Conservative learning rate\u001b[39;49;00m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_swa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    727\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸŽ¯ Hybrid Model Best Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhybrid_best_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    731\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;66;03m# BLOCK 15 â€” Comprehensive Evaluation\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 518\u001b[0m, in \u001b[0;36mtrain_comprehensive\u001b[1;34m(model, train_loader, val_loader, model_name, num_classes, epochs, lr, use_swa)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;66;03m# Advanced components\u001b[39;00m\n\u001b[0;32m    517\u001b[0m gradient_accumulator \u001b[38;5;241m=\u001b[39m GradientAccumulator(accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 518\u001b[0m swa \u001b[38;5;241m=\u001b[39m \u001b[43mStochasticWeightAveraging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswa_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m use_swa \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    520\u001b[0m history \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \n\u001b[0;32m    522\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rates\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m    524\u001b[0m }\n\u001b[0;32m    526\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 409\u001b[0m, in \u001b[0;36mStochasticWeightAveraging.__init__\u001b[1;34m(self, model, swa_start, swa_lr)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswa_lr \u001b[38;5;241m=\u001b[39m swa_lr\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswa_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mswa_utils\u001b[38;5;241m.\u001b[39mAveragedModel(model)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswa_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mswa_utils\u001b[38;5;241m.\u001b[39mSWALR(\n\u001b[1;32m--> 409\u001b[0m     \u001b[43moptimizers\u001b[49m, swa_lr\u001b[38;5;241m=\u001b[39mswa_lr\n\u001b[0;32m    410\u001b[0m )\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswa_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizers' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ============================================\n",
    "# MEDICAL VISION TRANSFORMER - Advanced Dermatology Classifier\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import timm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 1 â€” Setup & Diagnostics\n",
    "# ============================================\n",
    "\n",
    "# Paths\n",
    "ROOT_DIR = r\"C:\\Users\\anama\\Documents\\Group_8\"\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"Dataset\", \"DERM7PT\")\n",
    "META_CSV = os.path.join(DATASET_DIR, \"meta\", \"meta.csv\")\n",
    "IMAGES_FOLDER = os.path.join(DATASET_DIR, \"images\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 2 â€” Data Loading with Robust Validation\n",
    "# ============================================\n",
    "\n",
    "def load_and_validate_data():\n",
    "    \"\"\"Robust data loading with comprehensive validation\"\"\"\n",
    "    print(\"=== DATA LOADING & VALIDATION ===\")\n",
    "    \n",
    "    # Load metadata\n",
    "    df = pd.read_csv(META_CSV)\n",
    "    print(f\"Loaded meta.csv â€” shape: {df.shape}\")\n",
    "    \n",
    "    # Clean data\n",
    "    df = df.drop(columns=[\"case_num\", \"case_id\", \"notes\"], errors=\"ignore\")\n",
    "    df[\"derm_fullpath\"] = df[\"derm\"].apply(lambda x: os.path.join(IMAGES_FOLDER, x))\n",
    "    \n",
    "    # Validate paths\n",
    "    df[\"derm_exists\"] = df[\"derm_fullpath\"].apply(os.path.exists)\n",
    "    missing_count = (~df[\"derm_exists\"]).sum()\n",
    "    print(f\"Missing images: {missing_count}\")\n",
    "    \n",
    "    df = df[df[\"derm_exists\"]].reset_index(drop=True)\n",
    "    \n",
    "    # Filter classes with >= 2 samples\n",
    "    class_counts = df[\"diagnosis\"].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index\n",
    "    df = df[df[\"diagnosis\"].isin(valid_classes)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Final dataset size: {len(df)}\")\n",
    "    print(f\"Classes: {len(valid_classes)}\")\n",
    "    print(\"Class distribution:\")\n",
    "    print(class_counts[valid_classes])\n",
    "    \n",
    "    return df, valid_classes\n",
    "\n",
    "df, valid_classes = load_and_validate_data()\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"diagnosis\"])\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "class_names = le.classes_\n",
    "\n",
    "print(f\"\\nEncoded {NUM_CLASSES} classes:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"  {i}: {cls}\")\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 3 â€” Robust Data Split\n",
    "# ============================================\n",
    "\n",
    "def create_robust_splits(df):\n",
    "    \"\"\"Create splits with special handling for rare classes\"\"\"\n",
    "    # Handle melanoma metastasis separately\n",
    "    meta_mask = df[\"diagnosis\"] == \"melanoma metastasis\"\n",
    "    df_meta = df[meta_mask].copy()\n",
    "    df_main = df[~meta_mask].copy()\n",
    "    \n",
    "    print(f\"Main samples: {len(df_main)}, Metastasis: {len(df_meta)}\")\n",
    "    \n",
    "    # Main stratified split\n",
    "    train_main, temp_main = train_test_split(\n",
    "        df_main, test_size=0.30, stratify=df_main[\"label\"], random_state=42\n",
    "    )\n",
    "    val_main, test_main = train_test_split(\n",
    "        temp_main, test_size=0.50, stratify=temp_main[\"label\"], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Distribute metastasis samples\n",
    "    if len(df_meta) > 0:\n",
    "        df_meta_shuffled = df_meta.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        train_meta = df_meta_shuffled.iloc[:2] if len(df_meta) >= 2 else df_meta_shuffled.iloc[:1]\n",
    "        val_meta = df_meta_shuffled.iloc[2:3] if len(df_meta) >= 3 else pd.DataFrame()\n",
    "        test_meta = df_meta_shuffled.iloc[3:4] if len(df_meta) >= 4 else pd.DataFrame()\n",
    "    else:\n",
    "        train_meta, val_meta, test_meta = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # Final splits\n",
    "    train_df = pd.concat([train_main, train_meta]).reset_index(drop=True) if len(train_meta) > 0 else train_main\n",
    "    val_df = pd.concat([val_main, val_meta]).reset_index(drop=True) if len(val_meta) > 0 else val_main\n",
    "    test_df = pd.concat([test_main, test_meta]).reset_index(drop=True) if len(test_meta) > 0 else test_main\n",
    "    \n",
    "    print(f\"\\nFinal splits:\")\n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = create_robust_splits(df)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 4 â€” Medical-Optimized Augmentations\n",
    "# ============================================\n",
    "\n",
    "class MedicalAugmentation:\n",
    "    \"\"\"Medical-specific augmentations that preserve lesion characteristics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_train_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Less aggressive cropping\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomRotation(10),  # Conservative rotation\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),  # Subtle color changes\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_vit_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.02),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_test_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tta_transforms():\n",
    "        \"\"\"Test Time Augmentation transforms\"\"\"\n",
    "        return [\n",
    "            transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]),\n",
    "            transforms.Compose([transforms.Resize((224, 224)), transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor()]),\n",
    "            transforms.Compose([transforms.Resize((224, 224)), transforms.RandomVerticalFlip(p=1.0), transforms.ToTensor()]),\n",
    "        ]\n",
    "\n",
    "# Initialize transforms\n",
    "train_transform = MedicalAugmentation.get_train_transform()\n",
    "vit_transform = MedicalAugmentation.get_vit_transform()\n",
    "test_transform = MedicalAugmentation.get_test_transform()\n",
    "tta_transforms = MedicalAugmentation.get_tta_transforms()\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 5 â€” Robust Dataset with Debugging\n",
    "# ============================================\n",
    "\n",
    "class RobustDermDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, debug=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.debug = debug\n",
    "        \n",
    "        if debug:\n",
    "            self._validate_dataset()\n",
    "    \n",
    "    def _validate_dataset(self):\n",
    "        \"\"\"Comprehensive dataset validation\"\"\"\n",
    "        print(\"=== DATASET VALIDATION ===\")\n",
    "        print(f\"Total samples: {len(self.df)}\")\n",
    "        print(f\"Label range: {self.df['label'].min()} to {self.df['label'].max()}\")\n",
    "        print(f\"Num classes: {NUM_CLASSES}\")\n",
    "        \n",
    "        # Check for invalid labels\n",
    "        invalid_labels = self.df[self.df['label'] >= NUM_CLASSES]\n",
    "        if len(invalid_labels) > 0:\n",
    "            print(f\"ðŸš¨ ERROR: {len(invalid_labels)} invalid labels found!\")\n",
    "            print(invalid_labels[['diagnosis', 'label']].head())\n",
    "            raise ValueError(\"Invalid labels detected\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            row = self.df.iloc[idx]\n",
    "            img_path = str(row[\"derm_fullpath\"]).replace(\"/\", \"\\\\\")\n",
    "            label = int(row[\"label\"])\n",
    "            \n",
    "            # Validate label range\n",
    "            if label >= NUM_CLASSES or label < 0:\n",
    "                raise ValueError(f\"Invalid label {label} for sample {idx}\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            return img, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {e}\")\n",
    "            print(f\"Path: {img_path}, Label: {label}\")\n",
    "            raise e\n",
    "\n",
    "# Create datasets with validation\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_dataset = RobustDermDataset(train_df, transform=train_transform, debug=True)\n",
    "val_dataset = RobustDermDataset(val_df, transform=test_transform, debug=True)\n",
    "test_dataset = RobustDermDataset(test_df, transform=test_transform, debug=True)\n",
    "\n",
    "vit_train_dataset = RobustDermDataset(train_df, transform=vit_transform)\n",
    "vit_val_dataset = RobustDermDataset(val_df, transform=test_transform)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 6 â€” Advanced DataLoaders\n",
    "# ============================================\n",
    "\n",
    "def create_data_loaders():\n",
    "    \"\"\"Create optimized data loaders\"\"\"\n",
    "    # Weighted sampling for class imbalance\n",
    "    class_counts = train_df[\"label\"].value_counts().sort_index()\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = train_df[\"label\"].map(class_weights).values\n",
    "    sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "    \n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    batch_size = 16  # Reduced for stability\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    vit_train_loader = DataLoader(vit_train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "    vit_val_loader = DataLoader(vit_val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"DataLoader batch size: {batch_size}\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, vit_train_loader, vit_val_loader\n",
    "\n",
    "train_loader, val_loader, test_loader, vit_train_loader, vit_val_loader = create_data_loaders()\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 7 â€” Hybrid CNN-Transformer Model\n",
    "# ============================================\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    \"\"\"Combines CNN and Transformer features\"\"\"\n",
    "    def __init__(self, num_classes, cnn_dropout=0.2, transformer_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN backbone (ResNet50)\n",
    "        self.cnn_backbone = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "        self.cnn_backbone.fc = nn.Identity()\n",
    "        cnn_features = 2048\n",
    "        \n",
    "        # Transformer backbone (ViT)\n",
    "        self.transformer_backbone = timm.create_model(\n",
    "            \"vit_base_patch16_224\", \n",
    "            pretrained=True, \n",
    "            num_classes=0  # Remove classification head\n",
    "        )\n",
    "        transformer_features = 768\n",
    "        \n",
    "        # Feature fusion\n",
    "        total_features = cnn_features + transformer_features\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(total_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Proper weight initialization\"\"\"\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # CNN features\n",
    "        cnn_features = self.cnn_backbone(x)\n",
    "        \n",
    "        # Transformer features\n",
    "        transformer_features = self.transformer_backbone(x)\n",
    "        \n",
    "        # Feature fusion\n",
    "        combined_features = torch.cat([cnn_features, transformer_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(combined_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 8 â€” Advanced Loss Functions\n",
    "# ============================================\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing for better calibration\"\"\"\n",
    "    def __init__(self, smoothing=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=targets.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal loss for imbalanced datasets\"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 9 â€” Advanced Training Components\n",
    "# ============================================\n",
    "\n",
    "class StochasticWeightAveraging:\n",
    "    \"\"\"SWA for better generalization\"\"\"\n",
    "    def __init__(self, model, swa_start=10, swa_lr=1e-5):\n",
    "        self.model = model\n",
    "        self.swa_start = swa_start\n",
    "        self.swa_lr = swa_lr\n",
    "        self.swa_model = torch.optim.swa_utils.AveragedModel(model)\n",
    "        self.swa_scheduler = torch.optim.swa_utils.SWALR(\n",
    "            optimizers, swa_lr=swa_lr\n",
    "        )\n",
    "        self.swa_enabled = False\n",
    "    \n",
    "    def update(self, epoch):\n",
    "        if epoch >= self.swa_start and not self.swa_enabled:\n",
    "            self.swa_enabled = True\n",
    "            print(f\"ðŸš€ SWA activated at epoch {epoch}\")\n",
    "        \n",
    "        if self.swa_enabled:\n",
    "            self.swa_model.update_parameters(self.model)\n",
    "    \n",
    "    def swap_weights(self):\n",
    "        if self.swa_enabled:\n",
    "            self.swa_model.swap_swa_sgd()\n",
    "\n",
    "class GradientAccumulator:\n",
    "    \"\"\"Gradient accumulation for effective larger batch sizes\"\"\"\n",
    "    def __init__(self, accumulation_steps=4):\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.counter = 0\n",
    "    \n",
    "    def should_step(self):\n",
    "        self.counter += 1\n",
    "        return self.counter % self.accumulation_steps == 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 10 â€” Robust Training Loop\n",
    "# ============================================\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    \"\"\"Safe accuracy computation\"\"\"\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train_epoch_safe(model, loader, optimizer, criterion, device, gradient_accumulator):\n",
    "    \"\"\"Safe training with gradient checking\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    gradient_accumulator.reset()\n",
    "    \n",
    "    for batch_idx, (imgs, labels) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels) / gradient_accumulator.accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient accumulation step\n",
    "        if gradient_accumulator.should_step():\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulator.accumulation_steps\n",
    "        total_acc += compute_accuracy(outputs, labels)\n",
    "    \n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "def validate_epoch_safe(model, loader, criterion, device):\n",
    "    \"\"\"Safe validation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += compute_accuracy(outputs, labels)\n",
    "    \n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 11 â€” Comprehensive Training Manager\n",
    "# ============================================\n",
    "\n",
    "def train_comprehensive(model, train_loader, val_loader, model_name, num_classes, \n",
    "                       epochs=50, lr=1e-4, use_swa=True):\n",
    "    \"\"\"Comprehensive training with all advanced techniques\"\"\"\n",
    "    \n",
    "    # Loss function with label smoothing\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=1e-4,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=1e-7\n",
    "    )\n",
    "    \n",
    "    # Advanced components\n",
    "    gradient_accumulator = GradientAccumulator(accumulation_steps=4)\n",
    "    swa = StochasticWeightAveraging(model, swa_start=epochs//2) if use_swa else None\n",
    "    \n",
    "    history = {\n",
    "        \"train_loss\": [], \"val_loss\": [], \n",
    "        \"train_acc\": [], \"val_acc\": [],\n",
    "        \"learning_rates\": []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience, patience_counter = 10, 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epochs: {epochs}, LR: {lr}, Classes: {num_classes}\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch_safe(\n",
    "            model, train_loader, optimizer, criterion, device, gradient_accumulator\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = validate_epoch_safe(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # SWA update\n",
    "        if swa:\n",
    "            swa.update(epoch)\n",
    "        \n",
    "        # History tracking\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"learning_rates\"].append(current_lr)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "        print(f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"{model_name}_best.pth\")\n",
    "            print(f\"  âœ… New best model saved (val_acc: {best_val_acc:.4f})\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"ðŸ›‘ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Apply SWA at the end\n",
    "    if swa and swa.swa_enabled:\n",
    "        print(\"Applying SWA...\")\n",
    "        swa.swap_weights()\n",
    "        torch.save(model.state_dict(), f\"{model_name}_swa_final.pth\")\n",
    "    \n",
    "    return history, best_val_acc\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 12 â€” Test Time Augmentation (TTA)\n",
    "# ============================================\n",
    "\n",
    "def predict_with_tta(model, test_loader, device, num_transforms=5):\n",
    "    \"\"\"Test Time Augmentation for robust predictions\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=\"TTA Prediction\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            batch_probs = []\n",
    "            \n",
    "            # Original image\n",
    "            outputs = model(imgs)\n",
    "            batch_probs.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Augmented predictions\n",
    "            for i in range(num_transforms - 1):\n",
    "                if i < len(tta_transforms):\n",
    "                    augmented_imgs = tta_transforms[i](imgs.cpu()).to(device)\n",
    "                else:\n",
    "                    # Random augmentation for remaining transforms\n",
    "                    aug_transform = transforms.Compose([\n",
    "                        transforms.RandomHorizontalFlip(p=0.5),\n",
    "                        transforms.RandomVerticalFlip(p=0.3),\n",
    "                        transforms.ToTensor()\n",
    "                    ])\n",
    "                    augmented_imgs = torch.stack([aug_transform(img.cpu()) for img in imgs]).to(device)\n",
    "                \n",
    "                outputs = model(augmented_imgs)\n",
    "                batch_probs.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Average probabilities\n",
    "            avg_probs = torch.stack(batch_probs).mean(0)\n",
    "            preds = torch.argmax(avg_probs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(avg_probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_probs), np.array(all_labels)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 13 â€” Smart Ensemble\n",
    "# ============================================\n",
    "\n",
    "class SmartEnsemble:\n",
    "    def __init__(self, models, val_loader, device):\n",
    "        self.models = models\n",
    "        self.device = device\n",
    "        self.weights = self._compute_optimal_weights(val_loader)\n",
    "        print(f\"Ensemble weights: {self.weights}\")\n",
    "    \n",
    "    def _compute_optimal_weights(self, val_loader):\n",
    "        \"\"\"Compute optimal weights based on validation performance\"\"\"\n",
    "        model_accuracies = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in val_loader:\n",
    "                    imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(imgs)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += len(labels)\n",
    "            \n",
    "            accuracy = correct / total\n",
    "            model_accuracies.append(accuracy)\n",
    "        \n",
    "        # Convert accuracies to weights using softmax\n",
    "        acc_tensor = torch.tensor(model_accuracies)\n",
    "        weights = F.softmax(acc_tensor * 5, dim=0)  # Temperature scaling\n",
    "        return weights.numpy()\n",
    "    \n",
    "    def predict(self, test_loader, use_tta=True):\n",
    "        \"\"\"Ensemble prediction with optional TTA\"\"\"\n",
    "        all_preds, all_probs, all_labels = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in test_loader:\n",
    "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                batch_probs = None\n",
    "                \n",
    "                for i, model in enumerate(self.models):\n",
    "                    if use_tta:\n",
    "                        # TTA for each model\n",
    "                        model_probs = []\n",
    "                        for transform in tta_transforms:\n",
    "                            augmented_imgs = transform(imgs.cpu()).to(self.device)\n",
    "                            outputs = model(augmented_imgs)\n",
    "                            model_probs.append(F.softmax(outputs, dim=1))\n",
    "                        avg_model_probs = torch.stack(model_probs).mean(0)\n",
    "                    else:\n",
    "                        outputs = model(imgs)\n",
    "                        avg_model_probs = F.softmax(outputs, dim=1)\n",
    "                    \n",
    "                    weighted_probs = avg_model_probs * self.weights[i]\n",
    "                    \n",
    "                    if batch_probs is None:\n",
    "                        batch_probs = weighted_probs\n",
    "                    else:\n",
    "                        batch_probs += weighted_probs\n",
    "                \n",
    "                preds = torch.argmax(batch_probs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_probs.extend(batch_probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        return np.array(all_preds), np.array(all_probs), np.array(all_labels)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 14 â€” Model Training Execution\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIALIZING ADVANCED TRAINING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize Hybrid Model\n",
    "print(\"\\nCreating Hybrid CNN-Transformer Model...\")\n",
    "hybrid_model = HybridModel(num_classes=NUM_CLASSES).to(device)\n",
    "print(f\"Hybrid model parameters: {sum(p.numel() for p in hybrid_model.parameters()):,}\")\n",
    "\n",
    "# Train Hybrid Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: TRAINING HYBRID MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hybrid_history, hybrid_best_acc = train_comprehensive(\n",
    "    hybrid_model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    model_name=\"Hybrid_CNN_Transformer\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    epochs=60,  # More epochs for convergence\n",
    "    lr=5e-5,    # Conservative learning rate\n",
    "    use_swa=True\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Hybrid Model Best Validation Accuracy: {hybrid_best_acc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 15 â€” Comprehensive Evaluation\n",
    "# ============================================\n",
    "\n",
    "def comprehensive_evaluation(model, test_loader, model_name, use_tta=True):\n",
    "    \"\"\"Comprehensive model evaluation with metrics\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if use_tta:\n",
    "        print(\"Using Test Time Augmentation...\")\n",
    "        preds, probs, true_labels = predict_with_tta(model, test_loader, device)\n",
    "    else:\n",
    "        model.eval()\n",
    "        preds, probs, true_labels = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                batch_probs = F.softmax(outputs, dim=1)\n",
    "                batch_preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                preds.extend(batch_preds.cpu().numpy())\n",
    "                probs.extend(batch_probs.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        preds, probs, true_labels = np.array(preds), np.array(probs), np.array(true_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, preds)\n",
    "    \n",
    "    print(f\"Accuracy:           {accuracy:.4f}\")\n",
    "    print(f\"Balanced Accuracy:  {balanced_accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nðŸ“Š Classification Report:\")\n",
    "    print(classification_report(true_labels, preds, target_names=class_names))\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    print(\"\\nðŸŽ¯ Per-Class Accuracy:\")\n",
    "    for i, cls in enumerate(class_names):\n",
    "        class_mask = (true_labels == i)\n",
    "        if class_mask.sum() > 0:\n",
    "            class_acc = (preds[class_mask] == i).mean()\n",
    "            print(f\"  {cls:25s}: {class_acc:.4f} ({class_mask.sum()} samples)\")\n",
    "        else:\n",
    "            print(f\"  {cls:25s}: No test samples\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, balanced_accuracy, probs\n",
    "\n",
    "# Evaluate Hybrid Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING HYBRID MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hybrid_accuracy, hybrid_balanced, hybrid_probs = comprehensive_evaluation(\n",
    "    hybrid_model, test_loader, \"Hybrid CNN-Transformer\", use_tta=True\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 16 â€” Training Curves Visualization\n",
    "# ============================================\n",
    "\n",
    "def plot_comprehensive_history(history, title):\n",
    "    \"\"\"Plot training history with multiple subplots\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "    ax2.plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    ax3.plot(history['learning_rates'], color='purple', linewidth=2)\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss vs Accuracy\n",
    "    ax4.scatter(history['train_loss'], history['train_acc'], alpha=0.6, label='Train')\n",
    "    ax4.scatter(history['val_loss'], history['val_acc'], alpha=0.6, label='Val')\n",
    "    ax4.set_title('Loss vs Accuracy')\n",
    "    ax4.set_xlabel('Loss')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nðŸ“ˆ Plotting Training History...\")\n",
    "plot_comprehensive_history(hybrid_history, \"Hybrid CNN-Transformer\")\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 17 â€” Final Results and Summary\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ HYBRID CNN-TRANSFORMER PERFORMANCE:\")\n",
    "print(f\"   Test Accuracy:        {hybrid_accuracy:.4f}\")\n",
    "print(f\"   Balanced Accuracy:    {hybrid_balanced:.4f}\")\n",
    "\n",
    "# Compare with baseline (0.389 from previous)\n",
    "baseline_accuracy = 0.389\n",
    "improvement = ((hybrid_accuracy - baseline_accuracy) / baseline_accuracy) * 100\n",
    "\n",
    "print(f\"\\nðŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "print(f\"   Baseline Accuracy:    {baseline_accuracy:.4f}\")\n",
    "print(f\"   Current Accuracy:     {hybrid_accuracy:.4f}\")\n",
    "print(f\"   Improvement:          +{improvement:+.1f}%\")\n",
    "\n",
    "if hybrid_accuracy > 0.50:\n",
    "    print(f\"\\nðŸŽ‰ EXCELLENT! Significant improvement achieved!\")\n",
    "elif hybrid_accuracy > 0.45:\n",
    "    print(f\"\\nâœ… GOOD! Moderate improvement achieved!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Needs further optimization\")\n",
    "\n",
    "# Save final results\n",
    "results_summary = {\n",
    "    'Model': ['Hybrid_CNN_Transformer'],\n",
    "    'Accuracy': [hybrid_accuracy],\n",
    "    'Balanced_Accuracy': [hybrid_balanced],\n",
    "    'Improvement_Over_Baseline': [improvement]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "results_df.to_csv('advanced_model_results.csv', index=False)\n",
    "print(f\"\\nðŸ’¾ Results saved to 'advanced_model_results.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69970bc1-a263-4b3d-9a17-aecf900c2530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "=== DATA LOADING & VALIDATION ===\n",
      "Loaded meta.csv â€” shape: (1011, 19)\n",
      "Missing images: 0\n",
      "Final dataset size: 1010\n",
      "Classes: 19\n",
      "Class distribution:\n",
      "diagnosis\n",
      "clark nevus                     399\n",
      "melanoma (less than 0.76 mm)    102\n",
      "reed or spitz nevus              79\n",
      "melanoma (in situ)               64\n",
      "melanoma (0.76 to 1.5 mm)        53\n",
      "seborrheic keratosis             45\n",
      "basal cell carcinoma             42\n",
      "dermal nevus                     33\n",
      "vascular lesion                  29\n",
      "blue nevus                       28\n",
      "melanoma (more than 1.5 mm)      28\n",
      "lentigo                          24\n",
      "dermatofibroma                   20\n",
      "congenital nevus                 17\n",
      "melanosis                        16\n",
      "combined nevus                   13\n",
      "miscellaneous                     8\n",
      "recurrent nevus                   6\n",
      "melanoma metastasis               4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Encoded 19 classes:\n",
      "  0: basal cell carcinoma\n",
      "  1: blue nevus\n",
      "  2: clark nevus\n",
      "  3: combined nevus\n",
      "  4: congenital nevus\n",
      "  5: dermal nevus\n",
      "  6: dermatofibroma\n",
      "  7: lentigo\n",
      "  8: melanoma (0.76 to 1.5 mm)\n",
      "  9: melanoma (in situ)\n",
      "  10: melanoma (less than 0.76 mm)\n",
      "  11: melanoma (more than 1.5 mm)\n",
      "  12: melanoma metastasis\n",
      "  13: melanosis\n",
      "  14: miscellaneous\n",
      "  15: recurrent nevus\n",
      "  16: reed or spitz nevus\n",
      "  17: seborrheic keratosis\n",
      "  18: vascular lesion\n",
      "Main samples: 1006, Metastasis: 4\n",
      "\n",
      "Final splits:\n",
      "Train: 706, Val: 152, Test: 152\n",
      "\n",
      "Creating datasets...\n",
      "=== DATASET VALIDATION ===\n",
      "Total samples: 706\n",
      "Label range: 0 to 18\n",
      "Num classes: 19\n",
      "=== DATASET VALIDATION ===\n",
      "Total samples: 152\n",
      "Label range: 0 to 18\n",
      "Num classes: 19\n",
      "=== DATASET VALIDATION ===\n",
      "Total samples: 152\n",
      "Label range: 0 to 18\n",
      "Num classes: 19\n",
      "DataLoader batch size: 16\n",
      "Train batches: 45\n",
      "\n",
      "================================================================================\n",
      "INITIALIZING ADVANCED TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Creating Enhanced Vision Transformer Model...\n",
      "Enhanced ViT parameters: 86,330,131\n",
      "\n",
      "============================================================\n",
      "PHASE 1: TRAINING ENHANCED VISION TRANSFORMER\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING Enhanced_ViT\n",
      "============================================================\n",
      "Epochs: 60, LR: 2e-05, Classes: 19\n",
      "\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/45 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ============================================\n",
    "# MEDICAL VISION TRANSFORMER - Advanced Dermatology Classifier\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import timm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 1 â€” Setup & Diagnostics\n",
    "# ============================================\n",
    "\n",
    "# Paths\n",
    "ROOT_DIR = r\"C:\\Users\\anama\\Documents\\Group_8\"\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"Dataset\", \"DERM7PT\")\n",
    "META_CSV = os.path.join(DATASET_DIR, \"meta\", \"meta.csv\")\n",
    "IMAGES_FOLDER = os.path.join(DATASET_DIR, \"images\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 2 â€” Data Loading with Robust Validation\n",
    "# ============================================\n",
    "\n",
    "def load_and_validate_data():\n",
    "    \"\"\"Robust data loading with comprehensive validation\"\"\"\n",
    "    print(\"=== DATA LOADING & VALIDATION ===\")\n",
    "    \n",
    "    # Load metadata\n",
    "    df = pd.read_csv(META_CSV)\n",
    "    print(f\"Loaded meta.csv â€” shape: {df.shape}\")\n",
    "    \n",
    "    # Clean data\n",
    "    df = df.drop(columns=[\"case_num\", \"case_id\", \"notes\"], errors=\"ignore\")\n",
    "    df[\"derm_fullpath\"] = df[\"derm\"].apply(lambda x: os.path.join(IMAGES_FOLDER, x))\n",
    "    \n",
    "    # Validate paths\n",
    "    df[\"derm_exists\"] = df[\"derm_fullpath\"].apply(os.path.exists)\n",
    "    missing_count = (~df[\"derm_exists\"]).sum()\n",
    "    print(f\"Missing images: {missing_count}\")\n",
    "    \n",
    "    df = df[df[\"derm_exists\"]].reset_index(drop=True)\n",
    "    \n",
    "    # Filter classes with >= 2 samples\n",
    "    class_counts = df[\"diagnosis\"].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index\n",
    "    df = df[df[\"diagnosis\"].isin(valid_classes)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Final dataset size: {len(df)}\")\n",
    "    print(f\"Classes: {len(valid_classes)}\")\n",
    "    print(\"Class distribution:\")\n",
    "    print(class_counts[valid_classes])\n",
    "    \n",
    "    return df, valid_classes\n",
    "\n",
    "df, valid_classes = load_and_validate_data()\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"diagnosis\"])\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "class_names = le.classes_\n",
    "\n",
    "print(f\"\\nEncoded {NUM_CLASSES} classes:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"  {i}: {cls}\")\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 3 â€” Robust Data Split\n",
    "# ============================================\n",
    "\n",
    "def create_robust_splits(df):\n",
    "    \"\"\"Create splits with special handling for rare classes\"\"\"\n",
    "    # Handle melanoma metastasis separately\n",
    "    meta_mask = df[\"diagnosis\"] == \"melanoma metastasis\"\n",
    "    df_meta = df[meta_mask].copy()\n",
    "    df_main = df[~meta_mask].copy()\n",
    "    \n",
    "    print(f\"Main samples: {len(df_main)}, Metastasis: {len(df_meta)}\")\n",
    "    \n",
    "    # Main stratified split\n",
    "    train_main, temp_main = train_test_split(\n",
    "        df_main, test_size=0.30, stratify=df_main[\"label\"], random_state=42\n",
    "    )\n",
    "    val_main, test_main = train_test_split(\n",
    "        temp_main, test_size=0.50, stratify=temp_main[\"label\"], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Distribute metastasis samples\n",
    "    if len(df_meta) > 0:\n",
    "        df_meta_shuffled = df_meta.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        train_meta = df_meta_shuffled.iloc[:2] if len(df_meta) >= 2 else df_meta_shuffled.iloc[:1]\n",
    "        val_meta = df_meta_shuffled.iloc[2:3] if len(df_meta) >= 3 else pd.DataFrame()\n",
    "        test_meta = df_meta_shuffled.iloc[3:4] if len(df_meta) >= 4 else pd.DataFrame()\n",
    "    else:\n",
    "        train_meta, val_meta, test_meta = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # Final splits\n",
    "    train_df = pd.concat([train_main, train_meta]).reset_index(drop=True) if len(train_meta) > 0 else train_main\n",
    "    val_df = pd.concat([val_main, val_meta]).reset_index(drop=True) if len(val_meta) > 0 else val_main\n",
    "    test_df = pd.concat([test_main, test_meta]).reset_index(drop=True) if len(test_meta) > 0 else test_main\n",
    "    \n",
    "    print(f\"\\nFinal splits:\")\n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = create_robust_splits(df)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 4 â€” Medical-Optimized Augmentations\n",
    "# ============================================\n",
    "\n",
    "class MedicalAugmentation:\n",
    "    \"\"\"Medical-specific augmentations that preserve lesion characteristics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_train_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_vit_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.02),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_test_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tta_transforms():\n",
    "        \"\"\"Test Time Augmentation transforms\"\"\"\n",
    "        return [\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "        ]\n",
    "\n",
    "# Initialize transforms\n",
    "train_transform = MedicalAugmentation.get_train_transform()\n",
    "vit_transform = MedicalAugmentation.get_vit_transform()\n",
    "test_transform = MedicalAugmentation.get_test_transform()\n",
    "tta_transforms = MedicalAugmentation.get_tta_transforms()\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 5 â€” Robust Dataset with Debugging\n",
    "# ============================================\n",
    "\n",
    "class RobustDermDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, debug=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.debug = debug\n",
    "        \n",
    "        if debug:\n",
    "            self._validate_dataset()\n",
    "    \n",
    "    def _validate_dataset(self):\n",
    "        \"\"\"Comprehensive dataset validation\"\"\"\n",
    "        print(\"=== DATASET VALIDATION ===\")\n",
    "        print(f\"Total samples: {len(self.df)}\")\n",
    "        print(f\"Label range: {self.df['label'].min()} to {self.df['label'].max()}\")\n",
    "        print(f\"Num classes: {NUM_CLASSES}\")\n",
    "        \n",
    "        # Check for invalid labels\n",
    "        invalid_labels = self.df[self.df['label'] >= NUM_CLASSES]\n",
    "        if len(invalid_labels) > 0:\n",
    "            print(f\"ðŸš¨ ERROR: {len(invalid_labels)} invalid labels found!\")\n",
    "            print(invalid_labels[['diagnosis', 'label']].head())\n",
    "            raise ValueError(\"Invalid labels detected\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            row = self.df.iloc[idx]\n",
    "            img_path = str(row[\"derm_fullpath\"]).replace(\"/\", \"\\\\\")\n",
    "            label = int(row[\"label\"])\n",
    "            \n",
    "            # Validate label range\n",
    "            if label >= NUM_CLASSES or label < 0:\n",
    "                raise ValueError(f\"Invalid label {label} for sample {idx}\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            return img, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {e}\")\n",
    "            print(f\"Path: {img_path}, Label: {label}\")\n",
    "            raise e\n",
    "\n",
    "# Create datasets with validation\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_dataset = RobustDermDataset(train_df, transform=train_transform, debug=True)\n",
    "val_dataset = RobustDermDataset(val_df, transform=test_transform, debug=True)\n",
    "test_dataset = RobustDermDataset(test_df, transform=test_transform, debug=True)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 6 â€” Advanced DataLoaders\n",
    "# ============================================\n",
    "\n",
    "def create_data_loaders():\n",
    "    \"\"\"Create optimized data loaders\"\"\"\n",
    "    # Weighted sampling for class imbalance\n",
    "    class_counts = train_df[\"label\"].value_counts().sort_index()\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = train_df[\"label\"].map(class_weights).values\n",
    "    sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "    \n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    batch_size = 16  # Reduced for stability\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"DataLoader batch size: {batch_size}\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders()\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 7 â€” Enhanced Vision Transformer Model\n",
    "# ============================================\n",
    "\n",
    "class EnhancedViT(nn.Module):\n",
    "    \"\"\"Enhanced Vision Transformer with better regularization\"\"\"\n",
    "    def __init__(self, num_classes, model_name='vit_base_patch16_224', dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained ViT\n",
    "        self.vit = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # Remove classification head\n",
    "            drop_rate=dropout\n",
    "        )\n",
    "        \n",
    "        # Get feature dimension\n",
    "        feature_dim = self.vit.num_features\n",
    "        \n",
    "        # Enhanced classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize classifier weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Proper weight initialization\"\"\"\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.vit(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 8 â€” Advanced Loss Functions\n",
    "# ============================================\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing for better calibration\"\"\"\n",
    "    def __init__(self, smoothing=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=targets.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 9 â€” Advanced Training Components\n",
    "# ============================================\n",
    "\n",
    "class GradientAccumulator:\n",
    "    \"\"\"Gradient accumulation for effective larger batch sizes\"\"\"\n",
    "    def __init__(self, accumulation_steps=4):\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.counter = 0\n",
    "    \n",
    "    def should_step(self):\n",
    "        self.counter += 1\n",
    "        return self.counter % self.accumulation_steps == 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 10 â€” Robust Training Loop\n",
    "# ============================================\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    \"\"\"Safe accuracy computation\"\"\"\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train_epoch_safe(model, loader, optimizer, criterion, device, gradient_accumulator):\n",
    "    \"\"\"Safe training with gradient checking\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    gradient_accumulator.reset()\n",
    "    \n",
    "    for batch_idx, (imgs, labels) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels) / gradient_accumulator.accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient accumulation step\n",
    "        if gradient_accumulator.should_step():\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulator.accumulation_steps\n",
    "        total_acc += compute_accuracy(outputs, labels)\n",
    "    \n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "def validate_epoch_safe(model, loader, criterion, device):\n",
    "    \"\"\"Safe validation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += compute_accuracy(outputs, labels)\n",
    "    \n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 11 â€” Comprehensive Training Manager\n",
    "# ============================================\n",
    "\n",
    "def train_comprehensive(model, train_loader, val_loader, model_name, num_classes, \n",
    "                       epochs=50, lr=1e-4, use_swa=True):\n",
    "    \"\"\"Comprehensive training with all advanced techniques\"\"\"\n",
    "    \n",
    "    # Loss function with label smoothing\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=1e-4,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=1e-7\n",
    "    )\n",
    "    \n",
    "    # SWA setup\n",
    "    if use_swa:\n",
    "        swa_model = AveragedModel(model)\n",
    "        swa_scheduler = SWALR(optimizer, swa_lr=1e-5)\n",
    "        swa_start = epochs // 2\n",
    "    else:\n",
    "        swa_model = None\n",
    "    \n",
    "    # Gradient accumulation\n",
    "    gradient_accumulator = GradientAccumulator(accumulation_steps=4)\n",
    "    \n",
    "    history = {\n",
    "        \"train_loss\": [], \"val_loss\": [], \n",
    "        \"train_acc\": [], \"val_acc\": [],\n",
    "        \"learning_rates\": []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience, patience_counter = 10, 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epochs: {epochs}, LR: {lr}, Classes: {num_classes}\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch_safe(\n",
    "            model, train_loader, optimizer, criterion, device, gradient_accumulator\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = validate_epoch_safe(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # SWA update\n",
    "        if use_swa and epoch >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        \n",
    "        # History tracking\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"learning_rates\"].append(current_lr)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "        print(f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"{model_name}_best.pth\")\n",
    "            print(f\"  âœ… New best model saved (val_acc: {best_val_acc:.4f})\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"ðŸ›‘ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Apply SWA at the end\n",
    "    if use_swa and swa_model is not None:\n",
    "        print(\"Applying SWA...\")\n",
    "        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n",
    "        torch.save(swa_model.state_dict(), f\"{model_name}_swa_final.pth\")\n",
    "    \n",
    "    return history, best_val_acc\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 12 â€” Test Time Augmentation (TTA)\n",
    "# ============================================\n",
    "\n",
    "def predict_with_tta(model, test_loader, device, num_transforms=3):\n",
    "    \"\"\"Test Time Augmentation for robust predictions\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=\"TTA Prediction\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            batch_probs = []\n",
    "            \n",
    "            # Original image + augmented predictions\n",
    "            for i in range(num_transforms):\n",
    "                if i == 0:\n",
    "                    # Original image\n",
    "                    augmented_imgs = imgs\n",
    "                elif i == 1:\n",
    "                    # Horizontal flip\n",
    "                    augmented_imgs = torch.flip(imgs, [3])\n",
    "                else:\n",
    "                    # Vertical flip\n",
    "                    augmented_imgs = torch.flip(imgs, [2])\n",
    "                \n",
    "                outputs = model(augmented_imgs)\n",
    "                batch_probs.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Average probabilities\n",
    "            avg_probs = torch.stack(batch_probs).mean(0)\n",
    "            preds = torch.argmax(avg_probs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(avg_probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_probs), np.array(all_labels)\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 13 â€” Model Training Execution\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIALIZING ADVANCED TRAINING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize Enhanced ViT Model\n",
    "print(\"\\nCreating Enhanced Vision Transformer Model...\")\n",
    "vit_model = EnhancedViT(num_classes=NUM_CLASSES).to(device)\n",
    "print(f\"Enhanced ViT parameters: {sum(p.numel() for p in vit_model.parameters()):,}\")\n",
    "\n",
    "# Train Enhanced ViT Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: TRAINING ENHANCED VISION TRANSFORMER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vit_history, vit_best_acc = train_comprehensive(\n",
    "    vit_model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    model_name=\"Enhanced_ViT\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    epochs=60,\n",
    "    lr=2e-5,    # Lower LR for ViT\n",
    "    use_swa=True\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Enhanced ViT Best Validation Accuracy: {vit_best_acc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 14 â€” Comprehensive Evaluation\n",
    "# ============================================\n",
    "\n",
    "def comprehensive_evaluation(model, test_loader, model_name, use_tta=True):\n",
    "    \"\"\"Comprehensive model evaluation with metrics\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if use_tta:\n",
    "        print(\"Using Test Time Augmentation...\")\n",
    "        preds, probs, true_labels = predict_with_tta(model, test_loader, device)\n",
    "    else:\n",
    "        model.eval()\n",
    "        preds, probs, true_labels = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                batch_probs = F.softmax(outputs, dim=1)\n",
    "                batch_preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                preds.extend(batch_preds.cpu().numpy())\n",
    "                probs.extend(batch_probs.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        preds, probs, true_labels = np.array(preds), np.array(probs), np.array(true_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, preds)\n",
    "    \n",
    "    print(f\"Accuracy:           {accuracy:.4f}\")\n",
    "    print(f\"Balanced Accuracy:  {balanced_accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nðŸ“Š Classification Report:\")\n",
    "    print(classification_report(true_labels, preds, target_names=class_names))\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    print(\"\\nðŸŽ¯ Per-Class Accuracy:\")\n",
    "    for i, cls in enumerate(class_names):\n",
    "        class_mask = (true_labels == i)\n",
    "        if class_mask.sum() > 0:\n",
    "            class_acc = (preds[class_mask] == i).mean()\n",
    "            print(f\"  {cls:25s}: {class_acc:.4f} ({class_mask.sum()} samples)\")\n",
    "        else:\n",
    "            print(f\"  {cls:25s}: No test samples\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, balanced_accuracy, probs\n",
    "\n",
    "# Evaluate Enhanced ViT Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ENHANCED VISION TRANSFORMER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vit_accuracy, vit_balanced, vit_probs = comprehensive_evaluation(\n",
    "    vit_model, test_loader, \"Enhanced Vision Transformer\", use_tta=True\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 15 â€” Training Curves Visualization\n",
    "# ============================================\n",
    "\n",
    "def plot_comprehensive_history(history, title):\n",
    "    \"\"\"Plot training history with multiple subplots\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "    ax2.plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    ax3.plot(history['learning_rates'], color='purple', linewidth=2)\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss vs Accuracy\n",
    "    ax4.scatter(history['train_loss'], history['train_acc'], alpha=0.6, label='Train')\n",
    "    ax4.scatter(history['val_loss'], history['val_acc'], alpha=0.6, label='Val')\n",
    "    ax4.set_title('Loss vs Accuracy')\n",
    "    ax4.set_xlabel('Loss')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nðŸ“ˆ Plotting Training History...\")\n",
    "plot_comprehensive_history(vit_history, \"Enhanced Vision Transformer\")\n",
    "\n",
    "# ============================================\n",
    "# BLOCK 16 â€” Final Results and Summary\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ ENHANCED VISION TRANSFORMER PERFORMANCE:\")\n",
    "print(f\"   Test Accuracy:        {vit_accuracy:.4f}\")\n",
    "print(f\"   Balanced Accuracy:    {vit_balanced:.4f}\")\n",
    "\n",
    "# Compare with baseline (0.389 from previous)\n",
    "baseline_accuracy = 0.389\n",
    "improvement = ((vit_accuracy - baseline_accuracy) / baseline_accuracy) * 100\n",
    "\n",
    "print(f\"\\nðŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "print(f\"   Baseline Accuracy:    {baseline_accuracy:.4f}\")\n",
    "print(f\"   Current Accuracy:     {vit_accuracy:.4f}\")\n",
    "print(f\"   Improvement:          +{improvement:+.1f}%\")\n",
    "\n",
    "if vit_accuracy > 0.50:\n",
    "    print(f\"\\nðŸŽ‰ EXCELLENT! Significant improvement achieved!\")\n",
    "elif vit_accuracy > 0.45:\n",
    "    print(f\"\\nâœ… GOOD! Moderate improvement achieved!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Needs further optimization\")\n",
    "\n",
    "# Save final results\n",
    "results_summary = {\n",
    "    'Model': ['Enhanced_Vision_Transformer'],\n",
    "    'Accuracy': [vit_accuracy],\n",
    "    'Balanced_Accuracy': [vit_balanced],\n",
    "    'Improvement_Over_Baseline': [improvement]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "results_df.to_csv('enhanced_vit_results.csv', index=False)\n",
    "print(f\"\\nðŸ’¾ Results saved to 'enhanced_vit_results.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10673749-f2e6-4bac-92ac-b7844d8a70b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fusion-gpu]",
   "language": "python",
   "name": "conda-env-fusion-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
