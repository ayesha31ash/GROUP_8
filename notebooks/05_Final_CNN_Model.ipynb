{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc499c5-eae8-4ac0-a2de-66856e96830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META_CSV: C:\\Users\\anama\\Documents\\Group_8\\Dataset\\DERM7PT\\meta\\meta.csv\n",
      "IMAGES_DIR: C:\\Users\\anama\\Documents\\Group_8\\Dataset\\DERM7PT\\images\n",
      "Using device: cuda\n",
      "Loaded meta.csv — shape: (1011, 19)\n",
      "Images missing on disk: 0\n",
      "After cleaning — shape: (1011, 18)\n",
      "Sample rows:\n",
      "              diagnosis            derm\n",
      "0  basal cell carcinoma  NEL/Nel026.jpg\n",
      "1  basal cell carcinoma  NEL/Nel028.jpg\n",
      "2  basal cell carcinoma  NEL/Nel033.jpg\n",
      "3  basal cell carcinoma  NEL/Nel035.jpg\n",
      "4  basal cell carcinoma  NEL/Nel037.jpg\n",
      "\n",
      "Label encoding:\n",
      " 0 -> basal cell carcinoma\n",
      " 1 -> blue nevus\n",
      " 2 -> clark nevus\n",
      " 3 -> combined nevus\n",
      " 4 -> congenital nevus\n",
      " 5 -> dermal nevus\n",
      " 6 -> dermatofibroma\n",
      " 7 -> lentigo\n",
      " 8 -> melanoma\n",
      " 9 -> melanoma (0.76 to 1.5 mm)\n",
      "10 -> melanoma (in situ)\n",
      "11 -> melanoma (less than 0.76 mm)\n",
      "12 -> melanoma (more than 1.5 mm)\n",
      "13 -> melanoma metastasis\n",
      "14 -> melanosis\n",
      "15 -> miscellaneous\n",
      "16 -> recurrent nevus\n",
      "17 -> reed or spitz nevus\n",
      "18 -> seborrheic keratosis\n",
      "19 -> vascular lesion\n",
      "NUM_CLASSES: 20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 104\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_CLASSES:\u001b[39m\u001b[38;5;124m\"\u001b[39m, NUM_CLASSES)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Block 4 — Stratified Train/Val/Test Split (70/15/15)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m train_df, temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m val_df, test_df \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m    112\u001b[0m     temp_df,\n\u001b[0;32m    113\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.50\u001b[39m,\n\u001b[0;32m    114\u001b[0m     stratify\u001b[38;5;241m=\u001b[39mtemp_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    115\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mSEED\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== SPLIT SIZES =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\fusion-gpu\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\fusion-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2940\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2936\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[0;32m   2938\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m-> 2940\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2942\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m   2944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2945\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2946\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2947\u001b[0m     )\n\u001b[0;32m   2948\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\fusion-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1927\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m \n\u001b[0;32m   1899\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1926\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m-> 1927\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[0;32m   1928\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\fusion-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2342\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2340\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[0;32m   2341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2347\u001b[0m     )\n\u001b[0;32m   2349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m   2350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[0;32m   2353\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 05_Final_CNN_Model.ipynb  (exported as .py style)\n",
    "# Final multiclass CNN for Derm7pt — ResNet50 + Aug-D + Focal Loss\n",
    "# ============================================================\n",
    "\n",
    "# ============================\n",
    "# Block 1 — Imports & Config\n",
    "# ============================\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "# -------- Paths (adapt if needed) --------\n",
    "ROOT_DIR    = r\"C:\\Users\\anama\\Documents\\Group_8\"\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"Dataset\", \"DERM7PT\")\n",
    "META_CSV    = os.path.join(DATASET_DIR, \"meta\", \"meta.csv\")\n",
    "IMAGES_DIR  = os.path.join(DATASET_DIR, \"images\")\n",
    "\n",
    "print(\"META_CSV:\", META_CSV)\n",
    "print(\"IMAGES_DIR:\", IMAGES_DIR)\n",
    "\n",
    "# -------- Device + Seeds --------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ============================\n",
    "# Block 2 — Load & Clean Metadata\n",
    "# ============================\n",
    "df = pd.read_csv(META_CSV)\n",
    "print(\"Loaded meta.csv — shape:\", df.shape)\n",
    "\n",
    "# Drop clearly useless columns if present\n",
    "df = df.drop(columns=[\"case_num\", \"case_id\", \"notes\"], errors=\"ignore\")\n",
    "\n",
    "# Build full image path (assuming 'derm' column holds filenames)\n",
    "if \"derm\" not in df.columns:\n",
    "    raise ValueError(\"Expected a 'derm' column in meta.csv for image filenames.\")\n",
    "\n",
    "df[\"derm_fullpath\"] = df[\"derm\"].apply(\n",
    "    lambda x: os.path.join(IMAGES_DIR, str(x))\n",
    ")\n",
    "\n",
    "df[\"derm_exists\"] = df[\"derm_fullpath\"].apply(os.path.exists)\n",
    "missing = df[~df[\"derm_exists\"]]\n",
    "\n",
    "print(\"Images missing on disk:\", len(missing))\n",
    "if len(missing) > 0:\n",
    "    print(\"Sample missing rows:\")\n",
    "    print(missing[[\"derm\", \"diagnosis\"]].head())\n",
    "\n",
    "# Keep only rows with existing images and non-null diagnosis\n",
    "df = df[df[\"derm_exists\"]].copy()\n",
    "df = df[~df[\"diagnosis\"].isna()].copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"After cleaning — shape:\", df.shape)\n",
    "print(\"Sample rows:\")\n",
    "print(df[[\"diagnosis\", \"derm\"]].head())\n",
    "\n",
    "# ============================\n",
    "# Block 3 — Encode Labels\n",
    "# ============================\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"diagnosis\"])\n",
    "class_names = list(le.classes_)\n",
    "NUM_CLASSES = len(class_names)\n",
    "\n",
    "print(\"\\nLabel encoding:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"{i:2d} -> {cls}\")\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "\n",
    "# ============================\n",
    "# Block 4 — Stratified Train/Val/Test Split (70/15/15)\n",
    "# ============================\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.30,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"\\n===== SPLIT SIZES =====\")\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Val:   {len(val_df)}\")\n",
    "print(f\"Test:  {len(test_df)}\")\n",
    "\n",
    "print(\"\\nClass distribution (train):\")\n",
    "print(train_df[\"diagnosis\"].value_counts())\n",
    "\n",
    "# ============================\n",
    "# Block 5 — Augmentation (Aug-D) & Datasets\n",
    "# ============================\n",
    "IMG_SIZE = 256  # slightly larger than 224 to capture more detail\n",
    "\n",
    "# Strong but realistic dermoscopy augmentation\n",
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0), ratio=(0.75, 1.33)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.02\n",
    "    ),\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "    T.ToTensor(),\n",
    "    T.RandomErasing(p=0.3),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "valid_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "class DermDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[\"derm_fullpath\"]\n",
    "        label = int(row[\"label\"])\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "train_dataset = DermDataset(train_df, transform=train_transform)\n",
    "val_dataset   = DermDataset(val_df,   transform=valid_transform)\n",
    "test_dataset  = DermDataset(test_df,  transform=valid_transform)\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "\n",
    "# ============================\n",
    "# Block 6 — DataLoaders\n",
    "# ============================\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Block 7 — Class Weights (for Focal Loss)\n",
    "# ============================\n",
    "class_counts = train_df[\"label\"].value_counts().sort_index()\n",
    "print(\"\\nClass counts (train):\")\n",
    "print(class_counts)\n",
    "\n",
    "# Simple inverse-frequency weights\n",
    "inv_freq = 1.0 / class_counts\n",
    "weights = inv_freq / inv_freq.sum() * len(inv_freq)\n",
    "\n",
    "class_weights_tensor = torch.tensor(\n",
    "    weights.values,\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nClass weights (normalized):\")\n",
    "print(class_weights_tensor)\n",
    "\n",
    "# ============================\n",
    "# Block 8 — Focal Loss\n",
    "# ============================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-class Focal Loss with per-class alpha weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # tensor [num_classes]\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: [B, C], targets: [B]\n",
    "        log_probs = F.log_softmax(logits, dim=1)        # [B, C]\n",
    "        probs = log_probs.exp()                         # [B, C]\n",
    "\n",
    "        # Gather log-prob and prob of the true class\n",
    "        targets = targets.long()\n",
    "        log_p = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)  # [B]\n",
    "        p = probs.gather(1, targets.unsqueeze(1)).squeeze(1)          # [B]\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]  # [B]\n",
    "        else:\n",
    "            alpha_t = torch.ones_like(p)\n",
    "\n",
    "        loss = -alpha_t * (1 - p) ** self.gamma * log_p\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0)\n",
    "\n",
    "# ============================\n",
    "# Block 9 — Build ResNet50 Model\n",
    "# ============================\n",
    "# Use ImageNet-pretrained ResNet50\n",
    "try:\n",
    "    weights_enum = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "    base_model = models.resnet50(weights=weights_enum)\n",
    "except AttributeError:\n",
    "    # For older torchvision versions\n",
    "    base_model = models.resnet50(pretrained=True)\n",
    "\n",
    "in_features = base_model.fc.in_features\n",
    "base_model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Optimizer & scheduler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "LR = 1e-4\n",
    "EPOCHS = 60\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(\"\\nModel built: ResNet50 with\", NUM_CLASSES, \"classes\")\n",
    "\n",
    "# ============================\n",
    "# Block 10 — Training & Validation Loops\n",
    "# ============================\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return epoch_loss, epoch_acc, bal_acc, np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# ============================\n",
    "# Block 11 — Train Model with Best-Checkpoint Saving\n",
    "# ============================\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_bal_acc\": []\n",
    "}\n",
    "\n",
    "best_bal_acc = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(\" TRAINING STARTED\")\n",
    "print(\"==========================\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    val_loss, val_acc, val_bal_acc, _, _ = eval_one_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_bal_acc\"].append(val_bal_acc)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val BalAcc: {val_bal_acc:.4f}\")\n",
    "\n",
    "    # Save best model based on validation balanced accuracy\n",
    "    if val_bal_acc > best_bal_acc:\n",
    "        best_bal_acc = val_bal_acc\n",
    "        best_state_dict = model.state_dict().copy()\n",
    "        torch.save(best_state_dict, os.path.join(ROOT_DIR, \"resnet50_augD_best.pth\"))\n",
    "        print(f\"  -> New best model saved (Val BalAcc = {best_bal_acc:.4f})\")\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(\" TRAINING FINISHED\")\n",
    "print(\"==========================\")\n",
    "print(\"Best Val Balanced Accuracy:\", best_bal_acc)\n",
    "\n",
    "# Load best weights before testing\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    print(\"Best checkpoint loaded for testing.\")\n",
    "\n",
    "# ============================\n",
    "# Block 12 — Test Evaluation\n",
    "# ============================\n",
    "test_loss, test_acc, test_bal_acc, y_true, y_pred = eval_one_epoch(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(\"\\n===== TEST RESULTS =====\")\n",
    "print(f\"Test Loss:       {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy:   {test_acc:.4f}\")\n",
    "print(f\"Test BalAcc:     {test_bal_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=False,\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title(\"Confusion Matrix — ResNet50 Aug-D (Test)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# Block 13 — Plot Learning Curves\n",
    "# ============================\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs_range, history[\"val_loss\"],   label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curves\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, history[\"train_acc\"],      label=\"Train Acc\")\n",
    "plt.plot(epochs_range, history[\"val_acc\"],        label=\"Val Acc\")\n",
    "plt.plot(epochs_range, history[\"val_bal_acc\"],    label=\"Val BalAcc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy curves\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de8e28-25e8-406b-beb7-6060af7a71cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fusion-gpu]",
   "language": "python",
   "name": "conda-env-fusion-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
